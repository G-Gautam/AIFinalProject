{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import PIL\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "RezNetTransform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, images_folder, transform = RezNetTransform):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename = str(self.df.id[index])+'.jpg'\n",
    "        label = self.df.stable[index]\n",
    "        image = PIL.Image.open(os.path.join(self.images_folder, filename))\n",
    "        \n",
    "        sample = {'image': image, 'stable': label}\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(\"blocks-labels.csv\", './data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = 51276 - train_size\n",
    "train, test = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, batch_size=4, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet18(pretrained=True)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs, 2)\n",
    "resnet = resnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom CNN\n",
    "class BaseNet(nn.Module):   \n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(16)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(32)\n",
    "        self.conv3 = torch.nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = torch.nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.nn.functional.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.nn.functional.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseNet = BaseNet(224, 224, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224*224\n",
    "num_classes = 2\n",
    "\n",
    "class LogRegModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, input_size)\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogRegModel(\n",
      "  (linear): Linear(in_features=50176, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "logreg = LogRegModel().to(device)\n",
    "print(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50176]) torch.Size([2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0026,  0.0019, -0.0010,  ...,  0.0027, -0.0033, -0.0026],\n",
       "         [-0.0012,  0.0034,  0.0014,  ..., -0.0002, -0.0008, -0.0042]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0023, 0.0007], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(logreg.linear.weight.shape, logreg.linear.bias.shape)\n",
    "list(logreg.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss() SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c26519c6fe548e8863dd5f3d0fb19e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10255 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6352148056030273\n",
      "[1,     1] loss: 0.003\n",
      "51.04426574707031\n",
      "[1,   201] loss: 4.511\n",
      "13.00998592376709\n",
      "[1,   401] loss: 4.076\n",
      "3.8872718811035156\n",
      "[1,   601] loss: 3.487\n",
      "4.019809722900391\n",
      "[1,   801] loss: 1.854\n",
      "34.978641510009766\n",
      "[1,  1001] loss: 1.495\n",
      "4.43442440032959\n",
      "[1,  1201] loss: 1.446\n",
      "2.095283031463623\n",
      "[1,  1401] loss: 2.133\n",
      "9.603708267211914\n",
      "[1,  1601] loss: 3.512\n",
      "24.50957679748535\n",
      "[1,  1801] loss: 1.862\n",
      "0.0\n",
      "[1,  2001] loss: 4.715\n",
      "10.269353866577148\n",
      "[1,  2201] loss: 5.898\n",
      "13.207889556884766\n",
      "[1,  2401] loss: 1.997\n",
      "21.68103790283203\n",
      "[1,  2601] loss: 2.466\n",
      "16.98550796508789\n",
      "[1,  2801] loss: 3.664\n",
      "5.63859224319458\n",
      "[1,  3001] loss: 3.803\n",
      "5.370378494262695e-05\n",
      "[1,  3201] loss: 1.620\n",
      "4.830921649932861\n",
      "[1,  3401] loss: 2.335\n",
      "24.77782440185547\n",
      "[1,  3601] loss: 3.272\n",
      "12.501142501831055\n",
      "[1,  3801] loss: 2.609\n",
      "41.118927001953125\n",
      "[1,  4001] loss: 1.810\n",
      "26.28501319885254\n",
      "[1,  4201] loss: 3.693\n",
      "28.50503158569336\n",
      "[1,  4401] loss: 2.173\n",
      "6.046274662017822\n",
      "[1,  4601] loss: 1.608\n",
      "40.69782257080078\n",
      "[1,  4801] loss: 2.166\n",
      "16.699323654174805\n",
      "[1,  5001] loss: 1.850\n",
      "6.818587303161621\n",
      "[1,  5201] loss: 3.653\n",
      "0.0\n",
      "[1,  5401] loss: 2.924\n",
      "4.365367412567139\n",
      "[1,  5601] loss: 1.665\n",
      "0.07159628719091415\n",
      "[1,  5801] loss: 1.330\n",
      "0.27688348293304443\n",
      "[1,  6001] loss: 1.045\n",
      "3.1904635429382324\n",
      "[1,  6201] loss: 1.150\n",
      "0.8792776465415955\n",
      "[1,  6401] loss: 2.561\n",
      "7.8996782302856445\n",
      "[1,  6601] loss: 2.708\n",
      "54.138370513916016\n",
      "[1,  6801] loss: 3.988\n",
      "12.085992813110352\n",
      "[1,  7001] loss: 2.710\n",
      "33.52611541748047\n",
      "[1,  7201] loss: 3.600\n",
      "9.772665023803711\n",
      "[1,  7401] loss: 2.358\n",
      "77.95116424560547\n",
      "[1,  7601] loss: 1.783\n",
      "0.0\n",
      "[1,  7801] loss: 4.923\n",
      "13.058149337768555\n",
      "[1,  8001] loss: 4.368\n",
      "1.3232231140136719e-05\n",
      "[1,  8201] loss: 2.788\n",
      "4.188382148742676\n",
      "[1,  8401] loss: 2.007\n",
      "9.077945709228516\n",
      "[1,  8601] loss: 2.883\n",
      "1.12350332736969\n",
      "[1,  8801] loss: 5.420\n",
      "10.376986503601074\n",
      "[1,  9001] loss: 3.521\n",
      "13.306707382202148\n",
      "[1,  9201] loss: 3.422\n",
      "7.741745948791504\n",
      "[1,  9401] loss: 2.271\n",
      "0.0235862135887146\n",
      "[1,  9601] loss: 1.453\n",
      "1.8951630592346191\n",
      "[1,  9801] loss: 1.651\n",
      "6.390369415283203\n",
      "[1, 10001] loss: 2.098\n",
      "15.96751594543457\n",
      "[1, 10201] loss: 1.858\n",
      "Finished Training of RezNet\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer = optim.SGD(baseNet.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.SGD(logreg.parameters(), lr=0.001, momentum=0.9)\n",
    "# selected_model = resnet\n",
    "# selected_model = resnet\n",
    "selected_model = logreg\n",
    "print(criterion, optimizer)\n",
    "selected_model.to(device)\n",
    "losses = []\n",
    "for epoch in range(1):  # loop over the dataset\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(tqdm(train_loader)):\n",
    "        if i%5 != 0:\n",
    "            continue\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        # zero the parameter gradients\n",
    "        inputs, labels = data['image'].to(device), data['stable'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        output = selected_model(inputs)\n",
    "#         print(inputs.shape, labels.shape)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 0:   \n",
    "            print(loss.item())\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        losses.append(epoch_loss)\n",
    "print('Finished Training of RezNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6f2bc3486641e9a96f0bf2635311e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2564 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "1600\n",
      "2400\n",
      "3200\n",
      "4000\n",
      "4800\n",
      "5600\n",
      "6400\n",
      "7200\n",
      "8000\n",
      "8800\n",
      "9600\n",
      "Accuracy of the network on the test images: 52 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader):\n",
    "        i += 1\n",
    "        images, labels = data['image'].to(device), data['stable'].to(device)\n",
    "        outputs = selected_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if i % 200 == 0:   \n",
    "            print(total)\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bit0f725566702e4eb28298ae807491874a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
